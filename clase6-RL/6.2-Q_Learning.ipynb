{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Q-Learning.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B2PxweZ8yhAf",
        "colab_type": "text"
      },
      "source": [
        "## El objetivo de este ejercicio es implementar Q-learning\n",
        "<img src=\"https://github.com/javkrei/aprendizaje-reforzado-austral/blob/master/clase%205/Q-learning.PNG?raw=1\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzAdaSkN3uTB",
        "colab_type": "text"
      },
      "source": [
        "**Para evitar que tengan problemas importando cosas que están en repositorio, directamente pueden correr las siguientes dos celdas (que contienen lo que originalmente se iba a importar de la carpeta lib)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1gBN5aI-2c8Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "\n",
        "EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"])\n",
        "\n",
        "def plot_cost_to_go_mountain_car(env, estimator, num_tiles=20):\n",
        "    x = np.linspace(env.observation_space.low[0], env.observation_space.high[0], num=num_tiles)\n",
        "    y = np.linspace(env.observation_space.low[1], env.observation_space.high[1], num=num_tiles)\n",
        "    X, Y = np.meshgrid(x, y)\n",
        "    Z = np.apply_along_axis(lambda _: -np.max(estimator.predict(_)), 2, np.dstack([X, Y]))\n",
        "\n",
        "    fig = plt.figure(figsize=(10, 5))\n",
        "    ax = fig.add_subplot(111, projection='3d')\n",
        "    surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                           cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "    ax.set_xlabel('Position')\n",
        "    ax.set_ylabel('Velocity')\n",
        "    ax.set_zlabel('Value')\n",
        "    ax.set_title(\"Mountain \\\"Cost To Go\\\" Function\")\n",
        "    fig.colorbar(surf)\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_value_function(V, title=\"Value Function\"):\n",
        "    \"\"\"\n",
        "    Plots the value function as a surface plot.\n",
        "    \"\"\"\n",
        "    min_x = min(k[0] for k in V.keys())\n",
        "    max_x = max(k[0] for k in V.keys())\n",
        "    min_y = min(k[1] for k in V.keys())\n",
        "    max_y = max(k[1] for k in V.keys())\n",
        "\n",
        "    x_range = np.arange(min_x, max_x + 1)\n",
        "    y_range = np.arange(min_y, max_y + 1)\n",
        "    X, Y = np.meshgrid(x_range, y_range)\n",
        "\n",
        "    # Find value for all (x, y) coordinates\n",
        "    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))\n",
        "    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))\n",
        "\n",
        "    def plot_surface(X, Y, Z, title):\n",
        "        fig = plt.figure(figsize=(20, 10))\n",
        "        ax = fig.add_subplot(111, projection='3d')\n",
        "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
        "                               cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
        "        ax.set_xlabel('Player Sum')\n",
        "        ax.set_ylabel('Dealer Showing')\n",
        "        ax.set_zlabel('Value')\n",
        "        ax.set_title(title)\n",
        "        ax.view_init(ax.elev, -120)\n",
        "        fig.colorbar(surf)\n",
        "        plt.show()\n",
        "\n",
        "    plot_surface(X, Y, Z_noace, \"{} (No Usable Ace)\".format(title))\n",
        "    plot_surface(X, Y, Z_ace, \"{} (Usable Ace)\".format(title))\n",
        "\n",
        "\n",
        "\n",
        "def plot_episode_stats(stats, smoothing_window=10, noshow=False):\n",
        "    # Plot the episode length over time\n",
        "    fig1 = plt.figure(figsize=(10,5))\n",
        "    plt.plot(stats.episode_lengths)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Episode Length\")\n",
        "    plt.title(\"Episode Length over Time\")\n",
        "    if noshow:\n",
        "        plt.close(fig1)\n",
        "    else:\n",
        "        plt.show(fig1)\n",
        "\n",
        "    # Plot the episode reward over time\n",
        "    fig2 = plt.figure(figsize=(10,5))\n",
        "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
        "    plt.plot(rewards_smoothed)\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
        "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
        "    if noshow:\n",
        "        plt.close(fig2)\n",
        "    else:\n",
        "        plt.show(fig2)\n",
        "\n",
        "    # Plot time steps and episode number\n",
        "    fig3 = plt.figure(figsize=(10,5))\n",
        "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
        "    plt.xlabel(\"Time Steps\")\n",
        "    plt.ylabel(\"Episode\")\n",
        "    plt.title(\"Episode per time step\")\n",
        "    if noshow:\n",
        "        plt.close(fig3)\n",
        "    else:\n",
        "        plt.show(fig3)\n",
        "\n",
        "    return fig1, fig2, fig3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNfhzvvZ3lnR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Implementación del Cliff Walking\n",
        "\n",
        "import numpy as np\n",
        "import sys\n",
        "from gym.envs.toy_text import discrete\n",
        "\n",
        "\n",
        "UP = 0\n",
        "RIGHT = 1\n",
        "DOWN = 2\n",
        "LEFT = 3\n",
        "\n",
        "class CliffWalkingEnv(discrete.DiscreteEnv):\n",
        "\n",
        "    metadata = {'render.modes': ['human', 'ansi']}\n",
        "\n",
        "    def _limit_coordinates(self, coord):\n",
        "        coord[0] = min(coord[0], self.shape[0] - 1)\n",
        "        coord[0] = max(coord[0], 0)\n",
        "        coord[1] = min(coord[1], self.shape[1] - 1)\n",
        "        coord[1] = max(coord[1], 0)\n",
        "        return coord\n",
        "\n",
        "    def _calculate_transition_prob(self, current, delta):\n",
        "        new_position = np.array(current) + np.array(delta)\n",
        "        new_position = self._limit_coordinates(new_position).astype(int)\n",
        "        new_state = np.ravel_multi_index(tuple(new_position), self.shape)\n",
        "        reward = -100.0 if self._cliff[tuple(new_position)] else -1.0\n",
        "        is_done = self._cliff[tuple(new_position)] or (tuple(new_position) == (3,11))\n",
        "        return [(1.0, new_state, reward, is_done)]\n",
        "\n",
        "    def __init__(self):\n",
        "        self.shape = (4, 12)\n",
        "\n",
        "        nS = np.prod(self.shape)\n",
        "        nA = 4\n",
        "\n",
        "        # Cliff Location\n",
        "        self._cliff = np.zeros(self.shape, dtype=np.bool)\n",
        "        self._cliff[3, 1:-1] = True\n",
        "\n",
        "        # Calculate transition probabilities\n",
        "        P = {}\n",
        "        for s in range(nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            P[s] = { a : [] for a in range(nA) }\n",
        "            P[s][UP] = self._calculate_transition_prob(position, [-1, 0])\n",
        "            P[s][RIGHT] = self._calculate_transition_prob(position, [0, 1])\n",
        "            P[s][DOWN] = self._calculate_transition_prob(position, [1, 0])\n",
        "            P[s][LEFT] = self._calculate_transition_prob(position, [0, -1])\n",
        "\n",
        "        # We always start in state (3, 0)\n",
        "        isd = np.zeros(nS)\n",
        "        isd[np.ravel_multi_index((3,0), self.shape)] = 1.0\n",
        "\n",
        "        super(CliffWalkingEnv, self).__init__(nS, nA, P, isd)\n",
        "\n",
        "    def render(self, mode='human', close=False):\n",
        "        self._render(mode, close)\n",
        "\n",
        "    def _render(self, mode='human', close=False):\n",
        "        if close:\n",
        "            return\n",
        "\n",
        "        outfile = StringIO() if mode == 'ansi' else sys.stdout\n",
        "\n",
        "        for s in range(self.nS):\n",
        "            position = np.unravel_index(s, self.shape)\n",
        "            # print(self.s)\n",
        "            if self.s == s:\n",
        "                output = \" x \"\n",
        "            elif position == (3,11):\n",
        "                output = \" T \"\n",
        "            elif self._cliff[position]:\n",
        "                output = \" C \"\n",
        "            else:\n",
        "                output = \" o \"\n",
        "\n",
        "            if position[1] == 0:\n",
        "                output = output.lstrip() \n",
        "            if position[1] == self.shape[1] - 1:\n",
        "                output = output.rstrip() \n",
        "                output += \"\\n\"\n",
        "\n",
        "            outfile.write(output)\n",
        "        outfile.write(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vc7P4_dqyhAh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gym\n",
        "import itertools\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sys\n",
        "\n",
        "#if \"../\" not in sys.path:\n",
        "#  sys.path.append(\"../\") \n",
        "\n",
        "from collections import defaultdict\n",
        "#from lib.envs.cliff_walking import CliffWalkingEnv\n",
        "#from lib import plotting\n",
        "\n",
        "matplotlib.style.use('ggplot')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CUkKzAwkyhAq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = CliffWalkingEnv()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GR35DkB0yhAy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
        "    \"\"\"\n",
        "    Crea una política epsilon-greedy basado en una q-función (función de valor estado-acción) y un epsilon dados.\n",
        "    \n",
        "    Argumentos:\n",
        "        Q: un diccionario que mapea cada estado/observación s a un array de numpy Q[s] = array([v_0, v_1, ... , v_nA]) de longitud nA\n",
        "        que para un índice a del array contiene el valor v_a de tomar la acción a en el estado s. \n",
        "        (en nuestra notación de la clase q(s,a))\n",
        "         \n",
        "        epsilon: probabilidad de seleccionar una acción aleatoria (obliga a explorar), valor entre 0 y 1.\n",
        "        \n",
        "        nA: número de acciones en el entorno\n",
        "    \n",
        "    Retorna:\n",
        "        Una función que dada una observación como argumento, retorna una política (un array de numpy de longitud nA)\n",
        "        con probabilidades para cada acción. La política es tal que toma la mejor acción según Q con probabilidad (1-epsilon)\n",
        "        y toma una acción al azar con probabilidad epsilon \n",
        "    \"\"\"\n",
        "    \n",
        "    def policy_fn(observation):\n",
        "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
        "        best_action = np.argmax(Q[observation])\n",
        "        A[best_action] += (1.0 - epsilon)\n",
        "        return A\n",
        "    return policy_fn"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAt1jIiDyhA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 132
        },
        "outputId": "c5c878b3-f8c2-418e-8368-ebdbdd8aeda7"
      },
      "source": [
        "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
        "    \"\"\"\n",
        "    Algoritmo de Q-learning: Control TD off-policy. Encuentra la política greedy óptima mientras se comporta con una \n",
        "    política epsilon-greedy.\n",
        "    \n",
        "    Argumentos:\n",
        "        env: ambiente de OpenAI.\n",
        "        num_episodes: Número de episodios durante los cuales correr el algoritmo.\n",
        "        discount_factor: factor de descuento gama.\n",
        "        alpha: factor de aprendizaje TD.\n",
        "        epsilon: Probabilidad de elegir una acción aleatoria. Entre 0 y 1.\n",
        "    \n",
        "    Retorna:\n",
        "        Una tupla (Q, episode_lengths).\n",
        "        Q es la función de valor estado-acción óptima\n",
        "        Q es la función de valor estado-acción óptima, un diccionario que mapea estado -> array de valores para cada acción.\n",
        "        stats es un objeto EpisodeStats con dos arrays numpy para longitud de episodios y longitud de recompensas.\n",
        "    \"\"\"\n",
        "    \n",
        "    # La función de valor estado-acción final (q-función final)\n",
        "    #  un diccionario que mapea estado -> array de valores para cada acción.\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    # estadísticas útiles\n",
        "    stats = plotting.EpisodeStats(\n",
        "        episode_lengths=np.zeros(num_episodes),\n",
        "        episode_rewards=np.zeros(num_episodes))    \n",
        "    \n",
        "    # la política que seguimos\n",
        "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
        "    \n",
        "    for i_episode in range(num_episodes):\n",
        "        # printear cada 100 episodios\n",
        "        if (i_episode + 1) % 100 == 0:\n",
        "            print(\"\\rEpisodio {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
        "            sys.stdout.flush()\n",
        "            \n",
        "        # Resetear el ambiente y elegir una primera acción\n",
        "        state = env.reset()\n",
        "        \n",
        "        # Relalizar un paso en el ambiente\n",
        "        # total_reward = 0.0\n",
        "        for t in itertools.count():\n",
        "            \n",
        "            # Tomar un paso\n",
        "            action_probs = policy(state)\n",
        "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "            # Actualizar las estadísticas\n",
        "            stats.episode_rewards[i_episode] += reward\n",
        "            stats.episode_lengths[i_episode] = t\n",
        "            \n",
        "            # Actualización TD\n",
        "            best_next_action = np.argmax(Q[next_state])    \n",
        "            ####### COMPLETAR #########\n",
        "            Q[state][action] += \n",
        "                \n",
        "            if done:\n",
        "                break\n",
        "                \n",
        "            state = next_state\n",
        "    \n",
        "    return Q, stats"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-6-2dc0063bd509>\"\u001b[0;36m, line \u001b[0;32m57\u001b[0m\n\u001b[0;31m    Q[state][action] +=\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xl4x2pzZyhA8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q, stats = q_learning(env, 500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w-XWzagjyhBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plotting.plot_episode_stats(stats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tk2SO8rdyhBH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}